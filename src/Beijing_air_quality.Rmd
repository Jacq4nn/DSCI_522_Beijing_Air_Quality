---
title: "Exploratory data analysis of Beijing air quality data set"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message=FALSE)
library(here)
library(tidyverse)
library(knitr)
library(ggthemes)
theme_set(theme_minimal())
set.seed(2020)
```

# Summary of the data set

## Data Import

```{r load_data}
csv_list <- dir(here("data", "raw","PRSA_Data_20130301-20170228"))

air_data <- data.frame()
for (file_name in csv_list) {
        air_data <-
                rbind(air_data, 
                      read_csv(here("data", "raw", file_name), show_col_types = FALSE))
}

head(air_data)
```

The data set used in this project is an hourly air pollutants data from 12 nationally-controlled air-quality monitoring sites in Beijing, China from March 1st, 2013 to February 28th, 2017. The air-quality data are from the Beijing Municipal Environmental Monitoring Center. It was sourced from the UCI Machine Learning Repository [here](https://archive-beta.ics.uci.edu/ml/datasets/beijing+multi+site+air+quality+data).

Each row in the data set represents a measurements of air pollutants (e.g., PM2.5, PM10, CO) at specific date and time in 12 different district in Beijing, including Aotizhongxin, Changping, Dingling and so on. The meteorological data in each air-quality site are matched with the nearest weather station from the China Meteorological Administration.. There are `r nrow(air_data)` observations in the data set, and `r ncol(air_data) - 1` features. Below we show the structure of each features in the data set.

## Study the data

```{r study_data}
str(air_data)
```

# Partition the data set into training and test sets

Before proceeding further, we will split the data such that 75% of observations are in the training and 25% of observations are in the test set. Below we list the counts of observations for each class:

```{r split data}
# drop id and convert class to factor
bc_data <- bc_data %>% 
    select(-id) %>% 
    mutate(class = as.factor(class))
  
# split into training and test data sets
training_rows <- bc_data %>% 
    select(class) %>% 
    pull() %>%
    createDataPartition(p = 0.75, list = FALSE)
training_data <- bc_data %>% slice(training_rows)
test_data <- bc_data %>% slice(-training_rows)

train_counts <- summarise(training_data,
                          `Data partition` = "Training",
                          `Benign cases` = sum(class  == "B"),
                          `Malignant cases` = sum(class  == "M"))
test_counts <- summarise(test_data,
                         `Data partition` = "Test",
                         `Benign cases` = sum(class  == "B"),
                         `Malignant cases` = sum(class  == "M"))
kable(bind_rows(train_counts, test_counts),
      caption = "Table 2. Counts of observation for each class for each data partition.")
```

There is a minor class imbalance, but it is not so great that we will plan to immediately start our modeling plan with over- or under-sampling. If during initial tuning, there are indicators that it may in fact be a greater problem than anticipated (e.g., if the confusion matrix indicates that the model makes a lot more mistakes on the minority class, here malignant cases) then we will only then start to explore whether empoying techniques to address class imbalance may be of help to improving model performance in regards to predicting the minority class.

# Exploratory analysis on the training data set

To look at whether each of the predictors might be useful to predict the tumour class, we plotted the distributions of each predictor from the training data set and coloured the distribution by class (benign: blue and malignant: orange). In doing this we see that class distributions for all of the mean and max predictors for all the measurements overlap somewhat, but do show quite a difference in their centres and spreads. This is less so for the standard error (se) predictors. In particular, the standard errors of fractal dimension, smoothness, symmetry and texture look very similar in both the distribution centre and spread. Thus, we might choose to omit these from our model.

```{r predictor distributions, fig.width=8, fig.height=10, fig.cap="Distribution of training set predictors for the benign (B) and malignant (M) tumour cases."}
training_data %>% 
  gather(key = predictor, value = value, -class) %>% 
  mutate(predictor = str_replace_all(predictor, "_", " ")) %>% 
  ggplot(aes(x = value, y = class, colour = class, fill = class)) +
      facet_wrap(. ~ predictor, scale = "free", ncol = 4) +
      geom_density_ridges(alpha = 0.8) +
      scale_fill_tableau() +
      scale_colour_tableau() +
      guides(fill = FALSE, color = FALSE) +
      theme(axis.title.x = element_blank(),
            axis.title.y = element_blank())
```

Figure 1. Distribution of training set predictors for the benign (B) and malignant (M) tumour cases.

# References
